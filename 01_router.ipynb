{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vectordb_as_df(db_path:str):\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    for collection in client.list_collections():\n",
    "        data = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "        df = pd.DataFrame({\"ids\":data[\"ids\"], \n",
    "                        #    \"embeddings\":data[\"embeddings\"], \n",
    "                            \"metadatas\":data[\"metadatas\"], \n",
    "                            \"documents\":data[\"documents\"]})\n",
    "        df[\"first_div\"] = df[\"metadatas\"].apply(lambda x: x[\"First Division\"])\n",
    "        df[\"second_div\"] = df[\"metadatas\"].apply(lambda x: x[\"Second Division\"])\n",
    "        df[\"filename\"] = df[\"metadatas\"].apply(lambda x: x[\"File Name\"])\n",
    "        df = df[[\"ids\", \"first_div\", \"second_div\",\"filename\",\"documents\", \"metadatas\"]]\n",
    "    return df\n",
    "\n",
    "def get_first_div(db_path:str):\n",
    "    df = read_vectordb_as_df(db_path=db_path)\n",
    "    docs_list = df[\"first_div\"].unique().tolist()\n",
    "    docs_list.sort()\n",
    "    return docs_list\n",
    "\n",
    "def get_second_div(db_path:str):\n",
    "    df = read_vectordb_as_df(db_path=db_path)\n",
    "    docs_list = df[\"second_div\"].unique().tolist()\n",
    "    docs_list.sort()\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MANUAL',\n",
       " 'PORT',\n",
       " 'Rules',\n",
       " 'ABS',\n",
       " 'BV',\n",
       " 'Common',\n",
       " 'Cryostar',\n",
       " 'DNV',\n",
       " 'Integrated Smart Ship(ISS)',\n",
       " 'KR',\n",
       " 'MARPOL',\n",
       " 'Port Regulation',\n",
       " 'SOLAS',\n",
       " 'Win GD']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_path = \"./db/chroma_db_02\"\n",
    "rag_target1 = get_first_div(db_path=db_path)\n",
    "rag_target2 = get_second_div(db_path=db_path)\n",
    "rag_target = rag_target1 + rag_target2\n",
    "rag_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectordb, vectorstore, MANUAL, PORT, Rules, ABS, BV, Common, Cryostar, DNV, Integrated Smart Ship(ISS), KR, MARPOL, Port Regulation, SOLAS, Win GD'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_target.insert(0, \"vectorstore\")\n",
    "rag_target.insert(0, \"vectordb\")\n",
    "docs = \", \".join(rag_target)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\", \"database\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore or a database.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = f\"\"\"You are an expert at routing a user question to a vectorstore, web search or database.\n",
    "The vectorstore contains documents related to {docs}.\n",
    "Use the vectorstore for questions on these topics. \n",
    "The question contains words related to database.\n",
    "Use the database for questions on these topics. \n",
    "Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert at routing a user question to a vectorstore or web search or database.\\nThe vectorstore contains documents related to vectordb, vectorstore, MANUAL, PORT, Rules, ABS, BV, Common, Cryostar, DNV, Integrated Smart Ship(ISS), KR, MARPOL, Port Regulation, SOLAS, Win GD.\\nUse the vectorstore for questions on these topics. \\nThe question contains words related to database.\\nUse the database for questions on these topics. \\nOtherwise, use web-search.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D0A6CE6C60>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D0A6CE4920>, model_name='llama-3.2-11b-text-preview', temperature=1e-08, model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'properties': {'datasource': {'description': 'Given a user question choose to route it to web search or a vectorstore.', 'enum': ['vectorstore', 'web_search', 'database'], 'type': 'string'}}, 'required': ['datasource'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'RouteQuery'}}}, config={}, config_factories=[])\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(temperature=0, model_name= \"llama-3.2-11b-text-preview\")\n",
    "# llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"llama3.2:latest\")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "question_router = route_prompt | structured_llm_router\n",
    "question_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='database'\n",
      "datasource='web_search'\n",
      "datasource='database'\n",
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"what is eda?\"}))\n",
    "print(question_router.invoke({\"question\": \"according to vectorstore, what is eda?\"}))\n",
    "print(question_router.invoke({\"question\": \"according to rule, what is eda?\"}))  # 살짝 다르게\n",
    "print(question_router.invoke({\"question\": \"according to port regulation, what is eda?\"})) \n",
    "print(question_router.invoke({\"question\": \"according to menual, what is eda?\"}))  # 일부러 오타\n",
    "print(question_router.invoke({\"question\": \"about win gd, what is eda?\"})) \n",
    "print(question_router.invoke({\"question\": \"with reference to Rules, summaize RESOLUTION MEPC.248(66)\"}))\n",
    "print(question_router.invoke({\"question\": \"check the pressure in database\"}))\n",
    "print(question_router.invoke({\"question\": \"who is Son Heung-min\"}))\n",
    "print(question_router.invoke({\"question\": \"according to database, who is Son Heung-min\"}))\n",
    "print(question_router.invoke({\"question\": \"according to vectordb, who is Son Heung-min\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
