{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_ollama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OllamaEmbeddings\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, display\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from IPython.display import Image, display\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(temperature=0, model_name= \"llama-3.1-70b-versatile\")\n",
    "sql_llm = ChatGroq(model_name= \"llama-3.1-70b-versatile\")    \n",
    "\n",
    "# llm = ChatOllama(temperature=0, base_url=\"http://localhost:11434\", model=\"llama3.2:latest\")\n",
    "# sql_llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"llama3.2:latest\")\n",
    "\n",
    "### 올라마 도커 기동\n",
    "db_path = \"./db/chroma_db_02\"\n",
    "vectorstore = Chroma(collection_name=\"collection_01\", persist_directory=db_path, embedding_function=OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"bge-m3:latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_search_docs(query:str, vectorstore, k:int=100, fetch_k:int=200):\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\", \n",
    "        search_kwargs={'k': k, \"fetch_k\": fetch_k}\n",
    "        )\n",
    "    result = retriever.invoke(query)\n",
    "    return result\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def get_bm25_top_docs(query:str, documents:list, top_k:int=20):\n",
    "\n",
    "    tokenized_corpus = [doc.page_content for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    pattern = r'`(.*?)`'  # 백틱으로 둘러싸인 단어만 검색 대상으로 리스트에 담기\n",
    "    extracted_keywords = re.findall(pattern, query)\n",
    "\n",
    "    doc_scores = bm25.get_scores(extracted_keywords)\n",
    "    sorted_indices = np.argsort(doc_scores)  # 값에 대한 정렬된 인덱스\n",
    "    order_values = np.empty_like(sorted_indices)\n",
    "    order_values[sorted_indices] = np.arange(len(doc_scores))\n",
    "\n",
    "    top_index = [i for i, value in enumerate(order_values) if value < top_k]\n",
    "    top_docs = [i for i in documents if documents.index(i) in top_index ]\n",
    "    return top_docs\n",
    "    \n",
    "\n",
    "def get_keywords_matched_docs(query:str, documents:list, and_condition:bool=True):\n",
    "    pattern = r'`(.*?)`'  # 백틱으로 둘러싸인 단어만 검색 대상으로 리스트에 담기\n",
    "    extracted_keywords = re.findall(pattern, query)\n",
    "    lower_keywors = [keyword.lower() for keyword in extracted_keywords]\n",
    "\n",
    "    lower_docs = [doc.page_content.lower() for doc in documents]\n",
    "    if and_condition: matching_sentences = [sentence for sentence in lower_docs if all(keyword in sentence for keyword in lower_keywors)]\n",
    "    else: matching_sentences = [sentence for sentence in lower_docs if any(keyword in sentence for keyword in lower_keywors)]\n",
    "\n",
    "    matched_index = [lower_docs.index(doc) for doc in matching_sentences]\n",
    "    final_matched_docs = [documents[i] for i in matched_index]\n",
    "\n",
    "    return final_matched_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectordb_targets(db_path:str):\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    for collection in client.list_collections():\n",
    "        data = collection.get(include=['metadatas'])\n",
    "    lv1 = list(set([d['First Division'] for d in data[\"metadatas\"]]))\n",
    "    print(lv1)\n",
    "    lv2 = list(set([d['Second Division'] for d in data[\"metadatas\"]]))\n",
    "    print(lv2)\n",
    "    rag_target = lv1 + lv2\n",
    "    rag_target.insert(0, \"vectorstore\")\n",
    "    rag_target.insert(0, \"vectordb\")\n",
    "    docs = \", \".join(rag_target)\n",
    "    return docs\n",
    "\n",
    "docs = vectordb_targets(db_path=db_path)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model for Structured LLM\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"similarity_search\", \"vectorstore\", \"web_search\", \"database\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore or a similarity or a database.\",\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = f\"\"\"You are an expert at routing a user question to a vectorstore, web search or database.\n",
    "The vectorstore contains documents related to {docs}, Use the vectorstore for questions on these topics. \n",
    "The question contains words of similarity or sim search, Use similarity_search for the question.\n",
    "The question contains words related to database, Use the database for the question. \n",
    "Otherwise, use web-search.\"\"\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "question_router = route_prompt | structured_llm_router\n",
    "question_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_router.invoke({\"question\": \"who is Son Heung-min\"}))\n",
    "print(question_router.invoke({\"question\": \"according to database, who is Son Heung-min\"}))\n",
    "print(question_router.invoke({\"question\": \"according to vectorstore, who is Son Heung-min\"}))\n",
    "print(question_router.invoke({\"question\": \"sim search for Son Heung-min\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "retrieval_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt   (rlm/rag-prompt)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \n",
    "    \"\"\"You are a smart AI assistant. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    Generate detailed answer including specified numbers, fomulas in the point of technical specifications. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer:\"\"\"),\n",
    "    ])\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "rag_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Re-Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "%timeit\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "web_search_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        connection: web connection\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    connection: bool = False\n",
    "    question: str\n",
    "    generation: str   \n",
    "    documents: List[str]\n",
    "\n",
    "    sql_query: str\n",
    "    query_rows: list\n",
    "    attempts: int\n",
    "    relevance: str\n",
    "    sql_error: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_core.messages import RemoveMessage\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# Nodes\n",
    "\n",
    "def similarity_search(state):\n",
    "    \"\"\"\n",
    "    do hybrid similarity search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---Similarity Search---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    pattern = r'\"(.*?)\"'  # 따옴표로 둘러싸인 단어만 검색 대상으로 리스트에 담기\n",
    "    extracted_keywords = re.findall(pattern, question)\n",
    "    if len(extracted_keywords) > 0:\n",
    "        documents = get_semantic_search_docs(query=question, vectorstore=vectorstore, k=100, fetch_k=500)\n",
    "        print(len(documents))\n",
    "        documents = get_bm25_top_docs(query=question, documents=documents, top_k=10)\n",
    "        documents = get_keywords_matched_docs(query=question, documents=documents, and_condition=True)\n",
    "        print(len(documents))\n",
    "    else: documents = get_semantic_search_docs(query=question, vectorstore=vectorstore, k=3, fetch_k=500)\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    pattern = r'\"(.*?)\"'  # 따옴표로 둘러싸인 단어만 검색 대상으로 리스트에 담기\n",
    "    extracted_keywords = re.findall(pattern, question)\n",
    "    if len(extracted_keywords) > 0:\n",
    "        documents = get_semantic_search_docs(query=question, vectorstore=vectorstore, k=100, fetch_k=500)\n",
    "        documents = get_bm25_top_docs(query=question, documents=documents, top_k=10)\n",
    "        documents = get_keywords_matched_docs(query=question, documents=documents, and_condition=True)\n",
    "    else: documents = get_semantic_search_docs(query=question, vectorstore=vectorstore, k=3, fetch_k=500)\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    try:\n",
    "        print(f\"Count of Retrieved Docs: {len(documents)}\")\n",
    "        filtered_docs = []\n",
    "        for d in documents:\n",
    "            score = retrieval_grader.invoke(\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            grade = score.binary_score\n",
    "            if grade == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "                continue\n",
    "        return {\"documents\": filtered_docs, \"question\": question}\n",
    "    except Exception as e:\n",
    "        print(f\"Count of Retrieved Docs: {len(documents)}\")\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "def web_connection(state):\n",
    "    print(\"---WEB CONNECTION---\")\n",
    "    print(f\">>> state: {state}\")\n",
    "    res = check_internet(state)\n",
    "    return {\"connection\": res}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "def check_internet(state):\n",
    "    \"\"\"Check internet connection by pinging a website.\"\"\"\n",
    "    \n",
    "    print(\"---CHECK INTERNET CONNECTION---\")\n",
    "    # question = state[\"question\"]\n",
    "    try:\n",
    "        response = requests.get(\"https://www.google.com\", timeout=2)\n",
    "        # If the request is successful, we assume internet is ON\n",
    "        if response.status_code == 200:\n",
    "            # print(\"Internet is ON\")\n",
    "            return \"ON\"\n",
    "    except requests.ConnectionError:\n",
    "        # If there is a connection error, we assume internet is OFF\n",
    "        # print(\"Internet is OFF\")\n",
    "        return \"OFF\"\n",
    "    \n",
    "    \n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    try:\n",
    "        question = state[\"question\"]\n",
    "        source = question_router.invoke({\"question\": question})\n",
    "        if source.datasource == \"web_search\":\n",
    "            print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "            return \"web_search\"\n",
    "        elif source.datasource == \"vectorstore\":\n",
    "            print(\"---ROUTE QUESTION TO RAG---\")\n",
    "            return \"vectorstore\"\n",
    "        elif source.datasource == \"similarity_search\":\n",
    "            print(\"---ROUTE QUESTION TO SIMILARITY SEARCH---\")\n",
    "            return \"similarity_search\"\n",
    "        elif source.datasource == \"database\":\n",
    "            print(\"---ROUTE QUESTION TO DATABASE---\")\n",
    "            return \"database\"\n",
    "    except Exception as e:\n",
    "        print(f\"---ROUTING ERROR {e}---\")\n",
    "        return e\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION(Useful): GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION(Not Useful): GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION(Not Supported): GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Agent Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey, Float\n",
    "from sqlalchemy.orm import sessionmaker, relationship, declarative_base\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite:///./db/example.db\")\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Users table\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, index=True)\n",
    "    age = Column(Integer)\n",
    "    email = Column(String, unique=True, index=True)\n",
    "\n",
    "    orders = relationship(\"Order\", back_populates=\"user\")\n",
    "\n",
    "# Definition of the Food table\n",
    "class Food(Base):\n",
    "    __tablename__ = \"food\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, unique=True, index=True)\n",
    "    price = Column(Float)\n",
    "\n",
    "    orders = relationship(\"Order\", back_populates=\"food\")\n",
    "\n",
    "# Definition of the Orders table\n",
    "class Order(Base):\n",
    "    __tablename__ = \"orders\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    food_id = Column(Integer, ForeignKey(\"food.id\"))\n",
    "    user_id = Column(Integer, ForeignKey(\"users.id\"))\n",
    "\n",
    "    user = relationship(\"User\", back_populates=\"orders\")\n",
    "    food = relationship(\"Food\", back_populates=\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sqlalchemy import text, inspect\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_schema(engine):\n",
    "    inspector = inspect(engine)\n",
    "    schema = \"\"\n",
    "    for table_name in inspector.get_table_names():\n",
    "        schema += f\"Table: {table_name}\\n\"\n",
    "        for column in inspector.get_columns(table_name):\n",
    "            col_name = column[\"name\"]\n",
    "            col_type = str(column[\"type\"])\n",
    "            if column.get(\"primary_key\"):\n",
    "                col_type += \", Primary Key\"\n",
    "            if column.get(\"foreign_keys\"):\n",
    "                fk = list(column[\"foreign_keys\"])[0]\n",
    "                col_type += f\", Foreign Key to {fk.column.table.name}.{fk.column.name}\"\n",
    "            schema += f\"- {col_name}: {col_type}\\n\"\n",
    "        schema += \"\\n\"\n",
    "    print(\"Retrieved database schema.\")\n",
    "    return schema\n",
    "\n",
    "schema = get_database_schema(engine=engine)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckRelevance(BaseModel):\n",
    "    relevance: str = Field(\n",
    "        description=\"Indicates whether the question is related to the database schema. 'relevant' or 'not_relevant'.\"\n",
    "    )\n",
    "\n",
    "def check_relevance(state: GraphState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = get_database_schema(engine)\n",
    "    print(f\"Checking relevance of the question: {question}\")\n",
    "    system = \"\"\"You are an assistant that determines whether a given question is related to the following database schema.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Respond with only \"relevant\" or \"not_relevant\".\n",
    "\"\"\".format(schema=schema)\n",
    "    human = f\"Question: {question}\"\n",
    "    check_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", human),\n",
    "        ]\n",
    "    )\n",
    "    llm = sql_llm\n",
    "    structured_llm = llm.with_structured_output(CheckRelevance)\n",
    "    relevance_checker = check_prompt | structured_llm\n",
    "    relevance = relevance_checker.invoke({})\n",
    "    state[\"relevance\"] = relevance.relevance\n",
    "    print(f\"Relevance determined: {state['relevance']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToSQL(BaseModel):\n",
    "    sql_query: str = Field(\n",
    "        description=\"The SQL query corresponding to the natural language question.\"\n",
    "    )\n",
    "\n",
    "def convert_nl_to_sql(state: GraphState, config: RunnableConfig):\n",
    "    question = state[\"question\"]\n",
    "    schema = get_database_schema(engine)\n",
    "    print(f\"Converting question to SQL : {question}\")\n",
    "    system = f\"\"\"You are an assistant that converts natural language questions into SQL queries based on the following schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Provide only the SQL query without any explanations. \n",
    "Alias columns appropriately to match the expected keys in the result.\n",
    "\n",
    "For example, alias 'food.name' as 'food_name' and 'food.price' as 'price'.\n",
    "\"\"\"\n",
    "    convert_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"Question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = sql_llm\n",
    "    structured_llm = llm.with_structured_output(ConvertToSQL)\n",
    "    sql_generator = convert_prompt | structured_llm\n",
    "    result = sql_generator.invoke({\"question\": question})\n",
    "    state[\"sql_query\"] = result.sql_query\n",
    "    print(f\">>> Generated SQL query: {state['sql_query']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(state: GraphState):\n",
    "    sql_query = state[\"sql_query\"].strip()\n",
    "    session = SessionLocal()\n",
    "    print(f\"Executing SQL query: {sql_query}\")\n",
    "    try:\n",
    "        result = session.execute(text(sql_query))\n",
    "        if sql_query.lower().startswith(\"select\"):\n",
    "            rows = result.fetchall()\n",
    "            columns = result.keys()\n",
    "            if rows:\n",
    "                header = \", \".join(columns)\n",
    "                state[\"query_rows\"] = [dict(zip(columns, row)) for row in rows]\n",
    "                print(f\"Raw SQL Query Result: {state['query_rows']}\")\n",
    "                # Format the result for readability\n",
    "                data = \"; \".join([f\"{row.get('food_name', row.get('name'))} for ${row.get('price', row.get('food_price'))}\" for row in state[\"query_rows\"]])\n",
    "                formatted_result = f\"{header}\\n{data}\"\n",
    "            else:\n",
    "                state[\"query_rows\"] = []\n",
    "                formatted_result = \"No results found.\"\n",
    "            state[\"generation\"] = formatted_result\n",
    "            state[\"sql_error\"] = False\n",
    "            print(\"SQL SELECT query executed successfully.\")\n",
    "        else:\n",
    "            session.commit()\n",
    "            state[\"generation\"] = \"The action has been successfully completed.\"\n",
    "            state[\"sql_error\"] = False\n",
    "            print(\"SQL command executed successfully.\")\n",
    "    except Exception as e:\n",
    "        state[\"generation\"] = f\"Error executing SQL query: {str(e)}\"\n",
    "        state[\"sql_error\"] = True\n",
    "        print(f\"Error executing SQL query: {str(e)}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_readable_answer(state: GraphState):\n",
    "    sql = state[\"sql_query\"]\n",
    "    result = state[\"generation\"]\n",
    "    query_rows = state.get(\"query_rows\", [])\n",
    "    sql_error = state.get(\"sql_error\", False)\n",
    "    print(\"Generating a human-readable answer.\")\n",
    "    system = f\"\"\"You are an assistant that converts SQL query results into clear, natural language responses without including any identifiers like order IDs. \n",
    "    \"\"\"\n",
    "    if sql_error:\n",
    "        # Directly relay the error message\n",
    "        generate_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Formulate a clear and understandable error message in a single sentence, informing them about the issue.\"\"\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    elif sql.lower().startswith(\"select\"):\n",
    "        if not query_rows:\n",
    "            # Handle cases with no orders\n",
    "            generate_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system),\n",
    "                    (\n",
    "                        \"human\",\n",
    "                        f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Formulate a clear and understandable answer to the original question in a single sentence, and mention that there are no orders found.\"\"\"\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # Handle displaying orders\n",
    "            generate_prompt = ChatPromptTemplate.from_messages(\n",
    "                [\n",
    "                    (\"system\", system),\n",
    "                    (\n",
    "                        \"human\",\n",
    "                        f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Formulate a clear and understandable answer to the original question in a single sentence, and list each item ordered along with its price. \n",
    "For example: 'you have ordered Lasagne for $14.0 and Spaghetti Carbonara for $15.0.'\"\"\"\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        # Handle non-select queries\n",
    "        generate_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    f\"\"\"SQL Query:\n",
    "{sql}\n",
    "\n",
    "Result:\n",
    "{result}\n",
    "\n",
    "Formulate a clear and understandable confirmation message in a single sentence, confirming that your request has been successfully processed.\"\"\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    llm = sql_llm\n",
    "    human_response = generate_prompt | llm | StrOutputParser()\n",
    "    answer = human_response.invoke({})\n",
    "    state[\"generation\"] = answer\n",
    "    print(\"Generated human-readable answer.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewrittenQuestion(BaseModel):\n",
    "    question: str = Field(description=\"The rewritten question.\")\n",
    "\n",
    "def regenerate_query(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    print(\"Regenerating the SQL query by rewriting the question.\")\n",
    "    system = \"\"\"You are an assistant that reformulates an original question to enable more precise SQL queries. \n",
    "    Ensure that all necessary details, such as table joins, are preserved to retrieve complete and accurate data.\n",
    "    \"\"\"\n",
    "    rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                f\"Original Question: {question}\\nReformulate the question to enable more precise SQL queries, ensuring all necessary details are preserved.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    llm = sql_llm\n",
    "    structured_llm = llm.with_structured_output(RewrittenQuestion)\n",
    "    rewriter = rewrite_prompt | structured_llm\n",
    "    rewritten = rewriter.invoke({})\n",
    "    state[\"question\"] = rewritten.question\n",
    "    state[\"attempts\"] += 1\n",
    "    print(f\"Rewritten question: {state['question']}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_router(state: GraphState):\n",
    "    if state[\"relevance\"].lower() == \"relevant\":\n",
    "        return \"convert_to_sql\"\n",
    "    else:\n",
    "        return \"no_relevance\"\n",
    "\n",
    "def execute_sql_router(state: GraphState):\n",
    "    if not state.get(\"sql_error\", False):\n",
    "        return \"generate_human_readable_answer\"\n",
    "    else:\n",
    "        return \"regenerate_query\"\n",
    "\n",
    "def check_attempts_router(state: GraphState):\n",
    "    if state[\"attempts\"] < 3:\n",
    "        return \"convert_to_sql\"\n",
    "    else:\n",
    "        return \"end_max_iterations\"\n",
    "\n",
    "def end_max_iterations(state: GraphState):\n",
    "    state[\"generation\"] = \"Please try again.\"\n",
    "    print(\"Maximum attempts reached. Ending the workflow.\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.errors import GraphRecursionError\n",
    "%timeit\n",
    "# Run\n",
    "def app_stream(question:str, recursion_limit:int=10):\n",
    "    inputs = {\n",
    "        \"question\": question, \n",
    "        }\n",
    "    config = {\n",
    "        \"recursion_limit\": recursion_limit, \n",
    "        }\n",
    "    try:\n",
    "        for output in graph.stream(inputs, \n",
    "                                config, \n",
    "                                # stream_mode=\"debug\"\n",
    "                                ):\n",
    "            for key, value in output.items():\n",
    "                # Node\n",
    "                pprint(f\">>> Node : {key}\")\n",
    "            pprint(\"=\"*70)\n",
    "\n",
    "        # Final generation\n",
    "        print(\"\")\n",
    "        pprint(value)\n",
    "    except GraphRecursionError:\n",
    "        print(f\"=== Recursion Error - {recursion_limit} ===\")\n",
    "        value = f\"=== Recursion Error - {recursion_limit} ===\"\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_builder(state):\n",
    "    sql_builder = StateGraph(state)\n",
    "    sql_builder.add_node(\"check_relevance\", check_relevance)\n",
    "    sql_builder.add_node(\"convert_to_sql\", convert_nl_to_sql)\n",
    "    sql_builder.add_node(\"execute_sql\", execute_sql)\n",
    "    sql_builder.add_node(\"generate_human_readable_answer\", generate_human_readable_answer)\n",
    "    sql_builder.add_node(\"regenerate_query\", regenerate_query)\n",
    "    sql_builder.add_node(\"end_max_iterations\", end_max_iterations)\n",
    "\n",
    "    sql_builder.add_edge(START, \"check_relevance\")\n",
    "    sql_builder.add_conditional_edges(\n",
    "        \"check_relevance\",\n",
    "        relevance_router,\n",
    "        {\n",
    "            \"convert_to_sql\": \"convert_to_sql\",\n",
    "            \"no_relevance\": END,\n",
    "        },\n",
    "    )\n",
    "    sql_builder.add_edge(\"convert_to_sql\", \"execute_sql\")\n",
    "\n",
    "    sql_builder.add_conditional_edges(\n",
    "        \"execute_sql\",\n",
    "        execute_sql_router,\n",
    "        {\n",
    "            \"generate_human_readable_answer\": \"generate_human_readable_answer\",\n",
    "            \"regenerate_query\": \"regenerate_query\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    sql_builder.add_conditional_edges(\n",
    "        \"regenerate_query\",\n",
    "        check_attempts_router,\n",
    "        {\n",
    "            \"convert_to_sql\": \"convert_to_sql\",\n",
    "            \"max_iterations\": \"end_max_iterations\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    sql_builder.add_edge(\"generate_human_readable_answer\", END)\n",
    "    sql_builder.add_edge(\"end_max_iterations\", END)\n",
    "\n",
    "    return sql_builder.compile()\n",
    "\n",
    "graph = sql_builder(state=GraphState)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"from database, what is the lowest price food (name and price)\"\n",
    "result = app_stream(question=question, recursion_limit=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_builder(state):\n",
    "    sim_builder = StateGraph(state)\n",
    "    sim_builder.add_node(\"similarity_search\", similarity_search)  # similarity_search\n",
    "    sim_builder.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "    sim_builder.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "    sim_builder.add_edge(START, \"similarity_search\")\n",
    "    sim_builder.add_edge(\"similarity_search\", \"grade_documents\")\n",
    "    sim_builder.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": END,\n",
    "        },\n",
    "    )\n",
    "    sim_builder.add_edge(\"transform_query\", \"similarity_search\")\n",
    "    return sim_builder.compile()\n",
    "\n",
    "graph = sim_builder(state=GraphState)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'similarity search for the main function of the `noon report` in `iss` system'\n",
    "result = app_stream(question=question, recursion_limit=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search - Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_builder(state):\n",
    "    web_builder = StateGraph(state)\n",
    "    web_builder.add_node(\"web_connection\", web_connection)\n",
    "    web_builder.add_node(\"web_search\", web_search)\n",
    "    web_builder.add_node(\"generate\", generate)\n",
    "    web_builder.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "    web_builder.add_edge(START, \"web_connection\")\n",
    "    web_builder.add_conditional_edges(\n",
    "        \"web_connection\",\n",
    "        check_internet,\n",
    "        {\n",
    "            \"ON\": \"web_search\",\n",
    "            \"OFF\": END,\n",
    "        },\n",
    "    )\n",
    "    web_builder.add_edge(\"web_search\", \"generate\")\n",
    "    web_builder.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        grade_generation_v_documents_and_question,\n",
    "        {\n",
    "            \"not supported\": \"generate\",\n",
    "            \"useful\": END,\n",
    "            \"not useful\": \"transform_query\",\n",
    "        },\n",
    "    )\n",
    "    web_builder.add_edge(\"transform_query\", \"web_search\")\n",
    "    return web_builder.compile()\n",
    "\n",
    "graph = web_builder(state = GraphState)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Check today's weather condition of Seoul\"\n",
    "result = app_stream(question=question, recursion_limit=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_builder(state):\n",
    "    rag_builder = StateGraph(state)\n",
    "    rag_builder.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "    rag_builder.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "    rag_builder.add_node(\"generate\", generate)  # generatae\n",
    "    rag_builder.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "    rag_builder.add_edge(START, \"retrieve\")\n",
    "    rag_builder.add_edge(\"retrieve\", \"grade_documents\")\n",
    "    rag_builder.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        decide_to_generate,\n",
    "        {\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": \"generate\",\n",
    "        },\n",
    "    )\n",
    "    rag_builder.add_edge(\"transform_query\", \"retrieve\")\n",
    "    rag_builder.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        grade_generation_v_documents_and_question,\n",
    "        {\n",
    "            \"not supported\": \"generate\",\n",
    "            \"useful\": END,\n",
    "            \"not useful\": \"transform_query\",\n",
    "        },\n",
    "    )\n",
    "    return rag_builder.compile()\n",
    "\n",
    "graph = rag_builder(state=GraphState)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'according to `iss` manual, what is the `noon report` in iss system?'\n",
    "result = app_stream(question=question, recursion_limit=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 총조립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "def total_graph(state):\n",
    "    workflow = StateGraph(state)\n",
    "    workflow.add_node(\"retrieve\", retrieve)  \n",
    "    workflow.add_node(\"check_relevance\", check_relevance)\n",
    "    workflow.add_node(\"similarity_search\", similarity_search)  \n",
    "    workflow.add_node(\"web_connection\", web_connection)  \n",
    "\n",
    "    workflow.add_node(\"web_builder\", web_builder(state=state))\n",
    "    workflow.add_node(\"sql_builder\", sql_builder(state=state))\n",
    "    workflow.add_node(\"sim_builder\", sim_builder(state=state))\n",
    "    workflow.add_node(\"rag_builder\", rag_builder(state=state))\n",
    "\n",
    "    # Build graph\n",
    "    workflow.add_conditional_edges(\n",
    "        START,\n",
    "        route_question,\n",
    "        {\n",
    "            \"web_search\": \"web_connection\",\n",
    "            \"vectorstore\": \"retrieve\",\n",
    "            \"similarity_search\": \"similarity_search\",\n",
    "            \"database\": \"check_relevance\"\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"web_connection\", \"web_builder\")\n",
    "    workflow.add_edge(\"retrieve\", \"rag_builder\")\n",
    "    workflow.add_edge(\"similarity_search\", \"sim_builder\")\n",
    "    workflow.add_edge(\"check_relevance\", \"sql_builder\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "app = total_graph(state=GraphState)\n",
    "display(Image(app.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "from langgraph.errors import GraphRecursionError\n",
    "%timeit\n",
    "# Run\n",
    "def app_stream(question:str, recursion_limit:int=10):\n",
    "    inputs = {\n",
    "        \"question\": question, \n",
    "        }\n",
    "    config = {\n",
    "        \"recursion_limit\": recursion_limit, \n",
    "        # \"configurable\": {\"thread_id\": \"1\"}\n",
    "        }\n",
    "    try:\n",
    "        for output in app.stream(inputs, \n",
    "                                config, \n",
    "                                # stream_mode=\"debug\"\n",
    "                                ):\n",
    "            for key, value in output.items():\n",
    "                # Node\n",
    "                pprint(f\">>> Node : {key}\")\n",
    "            pprint(\"=\"*70)\n",
    "\n",
    "        # Final generation\n",
    "        print(\"\")\n",
    "        pprint(value)\n",
    "    except GraphRecursionError:\n",
    "        print(f\"=== Recursion Error - {recursion_limit} ===\")\n",
    "        value = f\"=== Recursion Error - {recursion_limit} ===\"\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'similarity search for the main function of the \"noon report\" in \"iss\" system'\n",
    "# question = 'sim search for the obligation of master of troubled vessel in \"singapore\" port'\n",
    "# question = 'similar search for the obligation of master of troubled vessel in \"singapore\" port'\n",
    "# question = '''\n",
    "# similarity search for below paragraph.\n",
    "\n",
    "# B 700 Owner's duties\n",
    "# 701 In order to maintain valid class the classification system specifies the following to be observed by the\n",
    "# owner:\n",
    "# — The ship has to be competently handled in accordance with the rules.\n",
    "# — The ship has to be maintained to rule standard at all times. Any conditions of class have to be carried out\n",
    "# as specified.\n",
    "# — The ship has to undergo prescribed periodical and renewal surveys, as well as surveys of damage, repairs,\n",
    "# conversions and alterations.\n",
    "# — The Society must be furnished with all information that may influence its decisions in connection with\n",
    "# classification.\n",
    "\n",
    "# '''\n",
    "\n",
    "question = 'according to `iss` manual, what is the `noon report` in `iss` system?'\n",
    "# question = 'according to \"iss\" manual, is the noon report editable?'\n",
    "\n",
    "# question = 'with reference to \"dnv\" rules, explain the \"noise\" level of \"radar\" rooms.(vectorstore)'\n",
    "# question = 'from the vectorstore, what is the obligation of master of troubled vessel in \"singapore\" port'\n",
    "\n",
    "# question = 'according to \"bv\" rule in vectorstore, explain the \"sloshing\" analysis model test in lng vessels'\n",
    "\n",
    "# question = 'check the \"presure\" of \"main engine\" from database'\n",
    "\n",
    "# question = \"Check today's weather condition of Seoul\"\n",
    "# question = \"explain me the basic concept of wig ship\"\n",
    "\n",
    "# question = \"from database, what is the lowest price food (name and price)\"\n",
    "# question = \"from database, check the weather condition\"\n",
    "\n",
    "\n",
    "result = app_stream(question=question, recursion_limit=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
